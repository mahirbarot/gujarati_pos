{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO13mY2kI8ICxzovJ2gevOL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahirbarot/gujarati_pos/blob/main/gujarati_pos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEWUebg2KZ3d"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename='/content/2_Gujarati Tourism POS Tagged Corpus.txt'\n",
        "with open(filename, 'r', encoding='utf-8') as f:\n",
        "    corpus = f.read()\n",
        "\n",
        "# new_data=re.sub(r'[^A-Z\\\\\\u0A80-\\u0AFF_ ]','',corpus)\n",
        "new_data=re.sub(r'[a-z(0-9)?]','',corpus)\n",
        "# new_data=re.sub(r'[\"IDVa-z0-9\"]','',corpus)\n",
        "#[\\u0900-\\u097F]\n",
        "# new_data=re.sub(r'[^_A-Z\\\\ ]','',corpus)\n",
        "#  = re.sub(r'[^]+', ' ', text)\n",
        "\n",
        "print(new_data)\n",
        "\n",
        "with open('data.txt','w') as f:\n",
        "  f.write(new_data)\n",
        "  f.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "NsH7obvKKhxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename='data.txt'\n",
        "# with open(filename, 'r', encoding='utf-8-sig') as f:\n",
        "with open(filename, 'r', encoding='utf-8-sig') as f:\n",
        "    clean_corpus = f.read().strip()\n",
        "    \n",
        "\n",
        "key_val=clean_corpus.split(\" \")"
      ],
      "metadata": {
        "id": "Y-dukc4WMd8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=[]\n",
        "y=[]\n",
        "for i in range(len(key_val)):\n",
        "  vals=key_val[i].split('\\\\')\n",
        "  if len(vals)==2:\n",
        "    vals[0]=re.sub('\\s+', '', vals[0])\n",
        "    vals[1]=re.sub('\\s+', '', vals[1])\n",
        "    x.append(vals[0])\n",
        "    y.append(vals[1])\n",
        "\n",
        "print(x[-5:])\n",
        "print(y[-5:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZoM68LnMs94",
        "outputId": "b3e880bb-b0b6-47c9-de61-e684344e8161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['સુરક્ષિત', 'રાખવામાં', 'આવી', 'છે', '']\n",
            "['JJ', 'V_VM', 'V_VAUX', 'V_VAUX', 'RD_PUNC']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.DataFrame(x,y)\n",
        "# df.to_csv('my_data.csv', index=False)\n",
        "print(x[0:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thu5iOmHOMG9",
        "outputId": "fe2aa731-f33e-408d-98c1-e8e26cda7ca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['સપ્તપુરીઓના', 'દર્શનથી', 'મળે', 'છે', 'તીર્થનું']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#code for converting lists to column\n",
        "percentile_list = pd.DataFrame(np.column_stack([x,y]), \n",
        "                               columns=['x','y'])\n",
        "print(x[0:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e5Kcw9TYUwZ",
        "outputId": "e0fe1faf-0a9b-499b-9131-4213b68cba71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['સપ્તપુરીઓના', 'દર્શનથી', 'મળે', 'છે', 'તીર્થનું']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('my_data.csv')\n",
        "df.head()\n",
        "percentile_list.tail()\n",
        "percentile_list.to_csv('clean_data_gujPOS.csv', index=False)"
      ],
      "metadata": {
        "id": "p8f1WjoSXUU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/clean_data_gujPOS.csv')\n",
        "x=df['x'].astype('str')\n",
        "y=df['y']"
      ],
      "metadata": {
        "id": "WomQGr5SZsyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the input data\n",
        "\n",
        "# Convert the labels to numeric categories\n",
        "y = np.squeeze(y)\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Tokenize the input data\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(x)\n",
        "x = tokenizer.texts_to_sequences(x)\n",
        "\n",
        "\n",
        "# Pad the sequences to a fixed length\n",
        "max_len = 50  # maximum length of a sentence\n",
        "x = pad_sequences(x, maxlen=max_len, padding='post')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the neural network model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64, input_length=max_len))\n",
        "model.add(LSTM(units=64, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "model.add(Dense(units=len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, np.array(y_train), epochs=10, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, np.array(y_test))\n",
        "print('Test accuracy:', test_acc)\n",
        "\n",
        "# Use the model to predict POS tags for new input text\n",
        "new_text = [...]  # list of new Gujarati sentences\n",
        "new_x = tokenizer.texts_to_sequences(new_text)\n",
        "new_x = pad_sequences(new_x, maxlen=max_len, padding='post')\n",
        "pred_y = model.predict(new_x)\n",
        "pred_y = [label_encoder.inverse_transform(np.argmax(pred, axis=-1)) for pred in pred_y]\n",
        "print('Predicted POS tags:', pred_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "LDbG3s--ZKMe",
        "outputId": "b7df6f1c-4613-4ead-e18e-0aa7da0b3b30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-9330d4dddeae>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36mfit_on_texts\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                     seq = text_to_word_sequence(\n\u001b[0m\u001b[1;32m    294\u001b[0m                         \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                         \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[0;34m(input_text, filters, lower, split)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \"\"\"\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mtranslate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
          ]
        }
      ]
    }
  ]
}